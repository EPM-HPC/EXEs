
Fatbin elf code:
================
arch = sm_80
code version = [1,7]
producer = <unknown>
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_80
code version = [7,1]
producer = <unknown>
host = linux
compile_size = 64bit
compressed








.version 7.1
.target sm_80
.address_size 64


.const .align 4 .b8 ConstArray1[1024];
.const .align 4 .b8 ConstArray2[1024];
.const .align 4 .b8 ConstArray3[1024];
.const .align 4 .b8 ConstArray4[1024];
.global .texref texmem1;
.global .texref texmem2;
.global .texref texmem3;

.visible .entry _Z11PowerKernalPji(
.param .u64 _Z11PowerKernalPji_param_0,
.param .u32 _Z11PowerKernalPji_param_1
)
{
.reg .pred %p<7>;
.reg .b32 %r<121>;
.reg .b64 %rd<24>;


ld.param.u64 %rd2, [_Z11PowerKernalPji_param_0];
ld.param.u32 %r9, [_Z11PowerKernalPji_param_1];
cvta.to.global.u64 %rd1, %rd2;
setp.eq.s32	%p1, %r9, 0;
@%p1 bra BB0_9;

mov.u32 %r14, %tid.x;
add.s32 %r1, %r14, 1;
and.b32 %r13, %r9, 3;
mov.u32 %r117, 0;
setp.eq.s32	%p2, %r13, 0;
@%p2 bra BB0_7;

setp.eq.s32	%p3, %r13, 1;
@%p3 bra BB0_6;

setp.eq.s32	%p4, %r13, 2;
@%p4 bra BB0_5;

and.b32 %r24, %r1, 255;
mul.wide.u32 %rd3, %r24, 4;
mov.u64 %rd4, ConstArray4;
add.s64 %rd5, %rd4, %rd3;
ld.const.u32 %r16, [%rd5];

	add.u32 %r15, %r16, 1;
add.u32 %r15, %r16, 1;


	
	add.u32 %r17, %r15, 5;
add.u32 %r17, %r15, 5;


	
	add.u32 %r19, %r17, 1;
add.u32 %r19, %r17, 1;


	
	add.u32 %r21, %r19, 5;
add.u32 %r21, %r19, 5;


	add.s32 %r25, %r17, %r15;
add.s32 %r26, %r25, %r19;
add.s32 %r27, %r26, %r21;
ld.global.u32 %r28, [%rd1];
add.s32 %r29, %r27, %r28;
st.global.u32 [%rd1], %r29;
mov.u32 %r117, 1;

BB0_5:
add.s32 %r38, %r1, %r117;
and.b32 %r39, %r38, 255;
mul.wide.u32 %rd6, %r39, 4;
mov.u64 %rd7, ConstArray4;
add.s64 %rd8, %rd7, %rd6;
ld.const.u32 %r31, [%rd8];

	add.u32 %r30, %r31, 1;
add.u32 %r30, %r31, 1;


	
	add.u32 %r32, %r30, 5;
add.u32 %r32, %r30, 5;


	
	add.u32 %r34, %r32, 1;
add.u32 %r34, %r32, 1;


	
	add.u32 %r36, %r34, 5;
add.u32 %r36, %r34, 5;


	add.s32 %r40, %r32, %r30;
add.s32 %r41, %r40, %r34;
add.s32 %r42, %r41, %r36;
ld.global.u32 %r43, [%rd1];
add.s32 %r44, %r42, %r43;
st.global.u32 [%rd1], %r44;
add.s32 %r117, %r117, 1;

BB0_6:
add.s32 %r53, %r1, %r117;
and.b32 %r54, %r53, 255;
mul.wide.u32 %rd9, %r54, 4;
mov.u64 %rd10, ConstArray4;
add.s64 %rd11, %rd10, %rd9;
ld.const.u32 %r46, [%rd11];

	add.u32 %r45, %r46, 1;
add.u32 %r45, %r46, 1;


	
	add.u32 %r47, %r45, 5;
add.u32 %r47, %r45, 5;


	
	add.u32 %r49, %r47, 1;
add.u32 %r49, %r47, 1;


	
	add.u32 %r51, %r49, 5;
add.u32 %r51, %r49, 5;


	add.s32 %r55, %r47, %r45;
add.s32 %r56, %r55, %r49;
add.s32 %r57, %r56, %r51;
ld.global.u32 %r58, [%rd1];
add.s32 %r59, %r57, %r58;
st.global.u32 [%rd1], %r59;
add.s32 %r117, %r117, 1;

BB0_7:
setp.lt.u32	%p5, %r9, 4;
@%p5 bra BB0_9;

BB0_8:
add.s32 %r92, %r1, %r117;
and.b32 %r93, %r92, 255;
mul.wide.u32 %rd12, %r93, 4;
mov.u64 %rd13, ConstArray4;
add.s64 %rd14, %rd13, %rd12;
ld.const.u32 %r61, [%rd14];

	add.u32 %r60, %r61, 1;
add.u32 %r60, %r61, 1;


	
	add.u32 %r62, %r60, 5;
add.u32 %r62, %r60, 5;


	
	add.u32 %r64, %r62, 1;
add.u32 %r64, %r62, 1;


	
	add.u32 %r66, %r64, 5;
add.u32 %r66, %r64, 5;


	add.s32 %r94, %r62, %r60;
add.s32 %r95, %r94, %r64;
add.s32 %r96, %r95, %r66;
ld.global.u32 %r97, [%rd1];
add.s32 %r98, %r96, %r97;
st.global.u32 [%rd1], %r98;
add.s32 %r99, %r92, 1;
mul.wide.u32 %rd15, %r99, 4;
and.b64 %rd16, %rd15, 1020;
add.s64 %rd17, %rd13, %rd16;
ld.const.u32 %r69, [%rd17];

	add.u32 %r68, %r69, 1;
add.u32 %r68, %r69, 1;


	
	add.u32 %r70, %r68, 5;
add.u32 %r70, %r68, 5;


	
	add.u32 %r72, %r70, 1;
add.u32 %r72, %r70, 1;


	
	add.u32 %r74, %r72, 5;
add.u32 %r74, %r72, 5;


	add.s32 %r100, %r70, %r68;
add.s32 %r101, %r100, %r72;
add.s32 %r102, %r101, %r74;
ld.global.u32 %r103, [%rd1];
add.s32 %r104, %r102, %r103;
st.global.u32 [%rd1], %r104;
add.s32 %r105, %r92, 2;
mul.wide.u32 %rd18, %r105, 4;
and.b64 %rd19, %rd18, 1020;
add.s64 %rd20, %rd13, %rd19;
ld.const.u32 %r77, [%rd20];

	add.u32 %r76, %r77, 1;
add.u32 %r76, %r77, 1;


	
	add.u32 %r78, %r76, 5;
add.u32 %r78, %r76, 5;


	
	add.u32 %r80, %r78, 1;
add.u32 %r80, %r78, 1;


	
	add.u32 %r82, %r80, 5;
add.u32 %r82, %r80, 5;


	add.s32 %r106, %r78, %r76;
add.s32 %r107, %r106, %r80;
add.s32 %r108, %r107, %r82;
ld.global.u32 %r109, [%rd1];
add.s32 %r110, %r108, %r109;
st.global.u32 [%rd1], %r110;
add.s32 %r111, %r92, 3;
mul.wide.u32 %rd21, %r111, 4;
and.b64 %rd22, %rd21, 1020;
add.s64 %rd23, %rd13, %rd22;
ld.const.u32 %r85, [%rd23];

	add.u32 %r84, %r85, 1;
add.u32 %r84, %r85, 1;


	
	add.u32 %r86, %r84, 5;
add.u32 %r86, %r84, 5;


	
	add.u32 %r88, %r86, 1;
add.u32 %r88, %r86, 1;


	
	add.u32 %r90, %r88, 5;
add.u32 %r90, %r88, 5;


	add.s32 %r112, %r86, %r84;
add.s32 %r113, %r112, %r88;
add.s32 %r114, %r113, %r90;
ld.global.u32 %r115, [%rd1];
add.s32 %r116, %r114, %r115;
st.global.u32 [%rd1], %r116;
add.s32 %r117, %r117, 4;
setp.lt.u32	%p6, %r117, %r9;
@%p6 bra BB0_8;

BB0_9:
ret;
}


